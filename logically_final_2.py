# -*- coding: utf-8 -*-
"""logically_final_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QvDITcjSskWR-TKc7QVn4sYbSAXytlq5

# Data Analysis - Consumer Complaint Database

## Imports
"""

!pip install plotly
!pip install tldextract -q
!pip install text2emotion

import sys
import os
import shutil
import math
import pandas as pd
import numpy as np
import pickle
import re
import gc
import random
import plotly.graph_objects as go
import itertools
from itertools import chain
from itertools import islice
import collections
import datetime, time
import warnings
warnings.filterwarnings("ignore")
import matplotlib
import matplotlib.image as mpimg
import matplotlib.pyplot as plt 
plt.style.use('seaborn-poster')  
matplotlib.rcParams['axes.labelsize'] = 18
matplotlib.rcParams['xtick.labelsize'] = 18
matplotlib.rcParams['ytick.labelsize'] = 18
matplotlib.rcParams['text.color'] = 'm'
import seaborn as sns
import plotly.graph_objects as go
import plotly.express as px
import plotly.io as pio
from plotly.subplots import make_subplots
 
import nltk
import tldextract
nltk.download('stopwords')    # Collection of all stopwords
nltk.download('punkt')     # collection fo all punctuations
nltk.download('wordnet')   # wordnet of NLP
from nltk.corpus import stopwords
stop_words = set(stopwords.words('english'))
from nltk import word_tokenize   # to tokenize sentense
from nltk.stem import WordNetLemmatizer# word lemmatization
lemmatizer = WordNetLemmatizer()
import text2emotion as te

import tensorflow as tf
import tensorflow_hub as hub
from tensorflow.python.util import deprecation
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras import models, layers, backend, metrics
from tensorflow.keras.callbacks import EarlyStopping

import keras
import keras.backend as K
import sklearn
from sklearn import preprocessing
from sklearn.utils import shuffle
from keras.models import Model
from keras.models import load_model
from keras.utils.vis_utils import plot_model
from keras.callbacks import ModelCheckpoint

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import normalize,LabelEncoder
from sklearn.feature_extraction.text import TfidfVectorizer    # for test vectorization TF-IDF
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer

from sklearn.decomposition import LatentDirichletAllocation
from sklearn.model_selection import KFold
from sklearn.model_selection import StratifiedKFold
from sklearn.linear_model import PassiveAggressiveClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB
from sklearn.neural_network import MLPClassifier
from sklearn.svm import SVC, LinearSVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import RidgeClassifier
from sklearn.linear_model import Perceptron
from sklearn.linear_model import SGDClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import VotingClassifier


from sklearn.pipeline import make_pipeline
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,confusion_matrix, classification_report

print('Preparing the pickle file.....')
pickle_inp_path='inp_data.pkl'

## Loading Data from csv-file

data=pd.read_csv("/content/sample_data/complaints_csv.csv")
data

pickle.dump((data),open(pickle_inp_path,'wb'))
print('Pickle files saved as ',pickle_inp_path)

## Loading Data from pickle-file

print('Loading the saved pickle files..')
data_input=pickle.load(open(pickle_inp_path, 'rb'))

data_input

try:
    data=data.drop('Unnamed: 0',axis=1)
except:
    pass
data.describe()
data.info()

# sub_sample1 = pd.DataFrame.sample(data, n=8, replace=False, weights=None, random_state=1, axis=1)
# sub_sample1

# data_input['Consumer complaint narrative']
print(data_input['Sub-issue'])

df=pd.DataFrame(data_input.columns)
print(df)

"""## Data Sampling"""

company_label=data_input['Company'].tolist()
company=data_input.Company.unique().tolist()
# type(company)
print(company)
# len(company_label)
Total_Companies=len(company)
Total_Companies

product_label=data_input['Product'].tolist()
product=data_input.Product.unique().tolist()
print(product)
# len(product_label)
Total_Products = len(product)
Total_Products

issue_label=data_input['Issue'].tolist()
issue=data_input.Issue.unique().tolist()
print(issue)
# issue
Number_of_Issues=len(issue)
Number_of_Issues

print('Preparing the pickle file for sub_sample.....')
pickle_inp_path='ss_data.pkl'

new_df=pd.DataFrame(data=[data_input['Company'],data_input['Product'],
                          data_input['Consumer complaint narrative'],data_input['Issue'], data_input['Sub-issue']])

pickle.dump((new_df),open(pickle_inp_path,'wb'))
print('Pickle files saved as ',pickle_inp_path)

print('Loading the saved pickle files..')
ss_data=pickle.load(open(pickle_inp_path, 'rb'))

sub_sample2=ss_data.transpose()
sub_sample3 = sub_sample2.sample(n=50000,axis=0,replace = True)
sub_sample3_skf= StratifiedKFold(n_splits=10, shuffle=True, random_state=1)
lst_accu_stratified = []
sub_sample3_skf

sscln=sub_sample3.fillna('XYZ')
sscln
df=pd.DataFrame(sscln)
df.to_csv("sub_sample.csv")

sscln.shape

allcompln=[]
for compln in sscln['Consumer complaint narrative']:
    allcompln.append([compln])

print(len(allcompln))
allcompln[4929]

"""## Applyimg NLP techniques for data cleaning / data pre-processing"""

def remove_stopwords(corpus, stop_words):
        return [tokens for tokens in corpus if not tokens in stop_words]

def remove_punctuations(stopwords_removed):
    return  [tokens for tokens in stopwords_removed if tokens.isalpha()]

def lower_case(no_punct):
    return  [tokens.lower() for tokens in no_punct]

def get_lemma(lower_tokens):
    return  [lemmatizer.lemmatize(tokens) for tokens in lower_tokens]

import itertools
from itertools import chain

def cleaning(corpus_text):
    cleaned_corpus=[]
    corpus = nltk.sent_tokenize(corpus_text)
    word_list = [word_tokenize(text) for text in corpus]
    corpus_lower = [token.lower() for token in corpus] # Lowercases the words.
    corpus_lower = [re.sub(r'\W',' ',token) for token in corpus_lower]  # Eliminates parentheses.
    corpus_lower = [re.sub(r'\s+',' ',token) for token in corpus_lower] # Eliminates double spaces.

    counter_word_list = []

    for sentence in corpus_lower:
        tokens = nltk.word_tokenize(sentence)
        for token in tokens:
            counter_word_list.append(token)
    stop_words = list(stopwords.words('english'))
    stopwords_removed =  [remove_stopwords(token, stop_words) for token in word_list]
    no_punct = [remove_punctuations(tokens) for tokens in stopwords_removed]
    lower_tokens = [lower_case(tokens) for tokens in no_punct]
    lemmatized_tokens = [get_lemma(tokens) for tokens in lower_tokens]
    new_wordlist = list(chain(*lemmatized_tokens))
    new_word_freq = Counter(new_wordlist)
    cleaned_corpus.append(new_wordlist)
    return cleaned_corpus

from collections import Counter
cleantext=[]
for i in allcompln:
    text = " ".join(str(x) for x in i)
    clean_text = cleaning(text)
    for y in clean_text:
        cleantext.append(' '.join(y).replace('xxxx', ''))  
# print(cleantext)
len(cleantext)

cleantext

textall=np.array(cleantext,str)
newtext=[]
for i in textall:
    if len(i) <= 3:
        pass
    else:
        newtext.append(i)

print(len(newtext))
print(newtext)

newtext[3][:1000]

"""# Sentiment Analysis"""

from textblob import TextBlob 

polarity=[]
subjectivity=[]
sentiment=[]
i=0
while i!=len(newtext):
    out=TextBlob(newtext[i]).sentiment
    polarity.append(out[0])
    if out[0]>0:
        sentiment.append("Positive")
    elif out[0]<0:
        sentiment.append("Negative")
    else:
        sentiment.append("Neutral")
    subjectivity.append(out[1])
    i+=1
senti_df=pd.DataFrame({
    "Consumer complaint narrative":newtext,
    "Polarity":polarity,
    "Subjectivity":subjectivity,
    "Sentiment":sentiment
})
print(len(senti_df))
senti_df.head()

senti_df['Consumer complaint narrative']

polarity=[]
subjectivity=[]
sentiment=[]
i=0
while i!=len(newtext):
    out=TextBlob(str(newtext[i])).sentiment
    polarity.append(out[0])
    if out[0]>0:
        sentiment.append("Positive")
    elif out[0]<0:
        sentiment.append("Negative")
    else:
        sentiment.append("Neutral")
    subjectivity.append(out[1])
    i+=1
print(len(polarity),len(subjectivity),len(sentiment),len(newtext))

customer_complaint_sentiment=pd.DataFrame({
    "Consumer complaint narrative":newtext,
    "Polarity":polarity,
    "Subjectivity":subjectivity,
    "Sentiment":sentiment
})
customer_complaint_sentiment=customer_complaint_sentiment.sample(n=len(customer_complaint_sentiment),random_state=42)
customer_complaint_sentiment=customer_complaint_sentiment.reset_index(drop=True)
customer_complaint_sentiment=customer_complaint_sentiment.fillna("Not Available")
customer_complaint_sentiment.to_csv("customer_complaint_sentiment.csv")
customer_complaint_sentiment.head()

sns.countplot(customer_complaint_sentiment.Sentiment,palette="Purples_r")

"""# Emotion Detection"""

customer_complaint_sentiment=customer_complaint_sentiment.reset_index(drop=True)
complaints=customer_complaint_sentiment['Consumer complaint narrative']
complaints=complaints.sample(n=1000)

emo=[]
for i in complaints:
    res=te.get_emotion(i)
    ky=list(res.keys())
    val=list(res.values())
    emo.append(ky[val.index(max(val))])
emotion_data=pd.DataFrame({"Complaints":complaints,"Emotion":emo})
emotion_data.to_csv("emotions_in_complaints.csv")
emotion_data=emotion_data[['Complaints','Emotion']]
print(emotion_data)

emo_label=[]
emo_count=[]
for i in emotion_data.Emotion.unique():
    df=emotion_data[emotion_data['Emotion']==i]
    lblcat=df['Emotion'].value_counts().index.tolist()
    emo_label.extend(lblcat)
    lblval=df['Emotion'].value_counts().tolist()
    emo_count.extend(lblval)
lbldf=pd.DataFrame({"Labels":emo_label,"Count":emo_count})
fig=px.bar(lbldf,x="Labels",y="Count",text="Count",color="Count",title="Count of Labels in Test Data")
fig.update_layout(
    font=dict(
        family="Times New Roman, Bold",
        size=20,
        color="purple"
    )
)
fig.show()

"""# Topic Modelling"""

import pandas as pd
import nltk
from nltk.corpus import stopwords  #stopwords
from nltk.stem import WordNetLemmatizer  
from sklearn.feature_extraction.text import TfidfVectorizer
stop_words=set(nltk.corpus.stopwords.words('english'))
from sklearn.decomposition import LatentDirichletAllocation

sscln

allissuefinal = [i for i in sscln['Issue']+' '+sscln['Sub-issue']]
allissuefinal

cleanissue=[]
for i in allissuefinal:
#     text = " ".join(str(x) for x in i)
    clean_issue = cleaning(i)
    for y in clean_issue:
        cleanissue.append(' '.join(y).replace('xxxx', ''))
len(cleanissue)

cleanissue

vect =TfidfVectorizer(stop_words=stop_words,max_features=5000)

vect_text=vect.fit_transform(cleanissue)

vect_text

lda_model=LatentDirichletAllocation(n_components=10,
learning_method='online',random_state=42,max_iter=1) 
lda_top=lda_model.fit_transform(vect_text)

print("Issues Document: ")
for i,topic in enumerate(lda_top[0]):
    print("Topic ",i,": ",topic*100,"%")

issuestopic = {}
vocab = vect.get_feature_names()
for i, comp in enumerate(lda_model.components_):
    vocab_comp = zip(vocab, comp)
    sorted_words = sorted(vocab_comp, key= lambda x:x[1], reverse=True)[:10]
    # print("\n"+"Topic "+str(i)+": ")
    issuestopic["Topic "+str(i)] = ' '.join([i[0] for i in sorted_words])
issuestopic

allcompln=[]
for compln in sscln['Consumer complaint narrative']:
    allcompln.append([compln])

allcompln

cleancompln=[]
for i in allcompln:
    text = " ".join(str(x) for x in i)
    clean_compln = cleaning(text)
    for y in clean_compln:
        cleancompln.append(' '.join(y).replace('xxxx', ''))
len(cleancompln)

cleancompln

vect =TfidfVectorizer(stop_words=stop_words,max_features=5000)

vect_text=vect.fit_transform(cleancompln)

vect_text

lda_model=LatentDirichletAllocation(n_components=10,
learning_method='online',random_state=42,max_iter=1) 
lda_top=lda_model.fit_transform(vect_text)

print("Complaints Document: ")
for i,topic in enumerate(lda_top[0]):
    print("Topic ",i,": ",topic*100,"%")

complaintstopic = {}
vocab = vect.get_feature_names()
for i, comp in enumerate(lda_model.components_):
    vocab_comp = zip(vocab, comp)
    sorted_words = sorted(vocab_comp, key= lambda x:x[1], reverse=True)[:10]
    complaintstopic["Topic "+str(i)] = ' '.join([i[0] for i in sorted_words])
complaintstopic
# complaintstopic = pd.DataFrame(complaintstopic, index=[0])
# complaintstopic

"""# Applying ML Classifiers"""

labels=customer_complaint_sentiment['Sentiment']
preds=customer_complaint_sentiment['Consumer complaint narrative']

skf = StratifiedKFold(n_splits=10, shuffle=False, random_state=None)
print(skf.get_n_splits(labels, preds))
for train_index, test_index in skf.split(labels, preds):
  print("TRAIN:", train_index, "TEST:", test_index)
  X_train, X_test = labels[train_index], labels[test_index]
  y_train, y_test = preds[train_index], preds[test_index]

# x_train,x_test,y_train,y_test=train_test_split(preds, labels, test_size=0.4, random_state=42)
tfidf_vectorizer=TfidfVectorizer(stop_words='english', max_df=0.9)
tfidf_train=tfidf_vectorizer.fit_transform(X_train) 
tfidf_test=tfidf_vectorizer.transform(X_test)

from statistics import *
sgd_accu_stratified = []
sgdce=SGDClassifier()
sgdce = sgdce.fit(tfidf_train,y_train)
predsgd = sgdce.predict(tfidf_test)
scoresgd=accuracy_score(y_test,predsgd)
print(f'Accuracy: {round(scoresgd*100,2)}%')

sgd_accu_stratified.append(sgdce.score(tfidf_test,y_test))
print('\nkfold_val_acc:',mean(sgd_accu_stratified)*100, '%')
# print('\nStandard Deviation is:', stdev(sgd_accu_stratified))

DT_accu_stratified=[]
DT = DecisionTreeClassifier()
DT.fit(tfidf_train, y_train)
preddt=DT.predict(tfidf_test)
scoredt=accuracy_score(y_test,preddt)
print(f'Accuracy: {round(scoredt*100,2)}%')

DT_accu_stratified.append(DT.score(tfidf_test,y_test))
print('\nkfold_val_acc:',mean(DT_accu_stratified)*100, '%')
# print('\nStandard Deviation is:', stdev(DT_accu_stratified))

RF_accu_stratified=[]
rf = RandomForestClassifier()
rf.fit(tfidf_train, y_train)
predrf=rf.predict(tfidf_test)
scorerf=accuracy_score(y_test,predrf)
print(f'Accuracy: {round(scorerf*100,2)}%')

RF_accu_stratified.append(rf.score(tfidf_test,y_test))
print('\nkfold_val_acc:',mean(RF_accu_stratified)*100, '%')

ridge_accu_stratified=[]
ridge = RidgeClassifier()
ridge.fit(tfidf_train, y_train)
predridge=ridge.predict(tfidf_test)
scoreridge=accuracy_score(y_test,predridge)
print(f'Accuracy: {round(scoreridge*100,2)}%')

ridge_accu_stratified.append(ridge.score(tfidf_test,y_test))
print('\nkfold_val_acc:',mean(ridge_accu_stratified)*100, '%')

labels=customer_complaint_sentiment['Sentiment']
preds=customer_complaint_sentiment['Consumer complaint narrative']
x_train,x_test,y_train,y_test=train_test_split(preds, labels, test_size=0.3, random_state=None)
tfidf_vectorizer=TfidfVectorizer(stop_words='english', max_df=0.9)
tfidf_train=tfidf_vectorizer.fit_transform(x_train) 
tfidf_test=tfidf_vectorizer.transform(x_test)

allscr=[]
prec=[]
rcl=[]
f1=[]
all_conf_mat=[]
algo_names=["Passive Aggresive Classifier",
            "Naïve Bayes Classifier"
      ]
algos=[PassiveAggressiveClassifier(random_state=0),
       MultinomialNB()]
for i in range(len(algos)):
    print("~~~~~~~~~~~~~~~~~~~~ Applying {} ~~~~~~~~~~~~~~~~~~\n".format(algo_names[i]))
    model = algos[i]
    model.fit(tfidf_train, y_train)
    modelpred=model.predict(tfidf_test)
    scoremodel=round(accuracy_score(y_test,modelpred),4)*100
    allscr.append(scoremodel)
    prec.append(round(precision_score(y_test,modelpred,average="micro"),4)*100)
    rcl.append(round(recall_score(y_test,modelpred,average="micro"),4)*100)
    # f1.append(round(f1_score(y_test,modelpred,average="micro"),4)*100)
    print("\tAccuracy for {} => {}%".format(algo_names[i],scoremodel))
    ct=pd.crosstab(y_test, modelpred, rownames=['True'], colnames=['Predicted'], margins=True)
    all_conf_mat.append(ct)
    print(classification_report(y_test,modelpred))
    plt.figure(figsize=(13,5))
    plt.title("Confusion Matrix for Stress Detection - {}".format(algo_names[i]), fontsize=20,color="#008000")
    sns.heatmap(ct.iloc[:2,:2],fmt="d",annot=True,cmap="Accent")
    plt.show()
    print()

all_conf_mat[0].iloc[:3,:3]

all_conf_mat[1].iloc[:3,:3]

"""## Applying ML Classifiers"""

precision=[]                    #Initialise empty lists for performance metrics
recall=[]
f1_score=[]
accuracy=[]
classifier_name=[]

# ### Data split and pre-processinglabels=customer_complaint_sentiment['Sentiment']
# preds=customer_complaint_sentiment['Consumer complaint narrative']

# skf = StratifiedKFold(n_splits=10, shuffle=False, random_state=None)
# print(skf.get_n_splits(labels, preds))
# for train_index, test_index in skf.split(labels, preds):
#   print("TRAIN:", train_index, "TEST:", test_index)
#   X_train, X_test = labels[train_index], labels[test_index]
#   y_train, y_test = preds[train_index], preds[test_index]

# # x_train,x_test,y_train,y_test=train_test_split(preds, labels, test_size=0.4, random_state=42)
# tfidf_vectorizer=TfidfVectorizer(stop_words='english', max_df=0.9)
# tfidf_train=tfidf_vectorizer.fit_transform(X_train) 
# tfidf_test=tfidf_vectorizer.transform(X_test)

labels=customer_complaint_sentiment['Sentiment']
preds=customer_complaint_sentiment['Consumer complaint narrative']
x_train,x_test,y_train,y_test=train_test_split(preds, labels, test_size=0.3, random_state=42)
tfidf_vectorizer=TfidfVectorizer(stop_words='english', max_df=0.9)
tfidf_train=tfidf_vectorizer.fit_transform(x_train) 
tfidf_test=tfidf_vectorizer.transform(x_test)

# ### Perceptron Classifier

perceptron = Perceptron(tol=1e-3)                              
perceptron = perceptron.fit(tfidf_train, y_train)
svpred = perceptron.predict(tfidf_test)
scoreperceptron=accuracy_score(y_test,svpred)
# print(f'Accuracy: {round(scoreperceptron*100,2)}%')

clsfr1 = pd.crosstab(y_test, svpred, rownames=['Actual'], colnames=['Predicted'], margins=True)

plt.figure(figsize=(5,5))
plt.title("Confusion Matrix (Perceptron)",fontsize=8)
sns.heatmap(clsfr1.iloc[:3,:3],cmap="Paired",fmt="d",annot=True)
precision.append(clsfr1.iloc[0,0]/(clsfr1.iloc[0,0]+clsfr1.iloc[0,1]))
recall.append(clsfr1.iloc[0,0]/(clsfr1.iloc[0,0]+clsfr1.iloc[1,0]))
f1_score.append(2*clsfr1.iloc[0,0]/(2*clsfr1.iloc[0,0]+clsfr1.iloc[0,1]+clsfr1.iloc[1,0]))
accuracy.append(round(scoreperceptron,2))
classifier_name.append("Perceptron")
clsfr1

# ### Stochastic Gradient Descent Classifier

sgdclfr=SGDClassifier()
sgdclfr = sgdclfr.fit(tfidf_train,y_train)
predsgd = sgdclfr.predict(tfidf_test)
scoresgd=accuracy_score(y_test,predsgd)
print(f'Accuracy: {round(scoresgd*100,2)}%')

clsfr2 = pd.crosstab(y_test, predsgd, rownames=['Actual'], colnames=['Predicted'], margins=True)
plt.figure(figsize=(5,5))
plt.title("Confusion Matrix (SGD_Classifier)",fontsize=8)
sns.heatmap(clsfr2.iloc[:3,:3],cmap="Paired",fmt="d",annot=True)
precision.append(clsfr2.iloc[0,0]/(clsfr2.iloc[0,0]+clsfr2.iloc[0,1]))
recall.append(clsfr2.iloc[0,0]/(clsfr2.iloc[0,0]+clsfr2.iloc[1,0]))
f1_score.append(2*clsfr2.iloc[0,0]/(2*clsfr2.iloc[0,0]+clsfr2.iloc[0,1]+clsfr2.iloc[1,0]))
accuracy.append(round(scoresgd,2))
classifier_name.append("SGD_Classifier")
clsfr2

# ### Naive Bias Classifier

G_NB = BernoulliNB()
G_NB = G_NB.fit(tfidf_train, y_train)
prednb=G_NB.predict(tfidf_test)
scorenb=accuracy_score(y_test,prednb)
print(f'Accuracy: {round(scorenb*100,2)}%')

clsfr3 = pd.crosstab(y_test, prednb, rownames=['Actual'], colnames=['Predicted'], margins=True)
plt.figure(figsize=(5,5))
plt.title("Confusion Matrix (Naive_Bias_Classifier)",fontsize=8)
sns.heatmap(clsfr3.iloc[:3,:3],cmap="Paired",fmt="d",annot=True)
precision.append(clsfr3.iloc[0,0]/(clsfr3.iloc[0,0]+clsfr3.iloc[0,1]))
recall.append(clsfr3.iloc[0,0]/(clsfr3.iloc[0,0]+clsfr3.iloc[1,0]))
f1_score.append(2*clsfr3.iloc[0,0]/(2*clsfr3.iloc[0,0]+clsfr3.iloc[0,1]+clsfr3.iloc[1,0]))
accuracy.append(round(scorenb,2))
classifier_name.append("Naive_Bias_Classifier")
clsfr3

# ### Decision Tree Classifier

DTclfr = DecisionTreeClassifier()
DTclfr = DTclfr.fit(tfidf_train, y_train)
preddt=DTclfr.predict(tfidf_test)
scoredt=accuracy_score(y_test,preddt)
print(f'Accuracy: {round(scoredt*100,2)}%')

clsfr4 = pd.crosstab(y_test, preddt, rownames=['Actual'], colnames=['Predicted'], margins=True)
plt.figure(figsize=(5,5))
plt.title("Confusion Matrix (Decision_Tree_Classifier)",fontsize=8)
sns.heatmap(clsfr4.iloc[:3,:3],cmap="Paired",fmt="d",annot=True)
precision.append(clsfr4.iloc[0,0]/(clsfr4.iloc[0,0]+clsfr4.iloc[0,1]))
recall.append(clsfr4.iloc[0,0]/(clsfr4.iloc[0,0]+clsfr4.iloc[1,0]))
f1_score.append(2*clsfr4.iloc[0,0]/(2*clsfr4.iloc[0,0]+clsfr4.iloc[0,1]+clsfr4.iloc[1,0]))
accuracy.append(round(scoredt,2))
classifier_name.append("Decision_Tree_Classifier")
clsfr4

# ### Random Forest Classifier

rfclfr = RandomForestClassifier()
rfclfr =rfclfr.fit(tfidf_train, y_train)
predrf=rfclfr.predict(tfidf_test)
scorerf=accuracy_score(y_test,predrf)
print(f'Accuracy: {round(scorerf*100,2)}%')

clsfr5 = pd.crosstab(y_test, predrf, rownames=['Actual'], colnames=['Predicted'], margins=True)
plt.figure(figsize=(5,5))
plt.title("Confusion Matrix (Random_Forest_Classifier)",fontsize=8)
sns.heatmap(clsfr5.iloc[:3,:3],cmap="Paired",fmt="d",annot=True)
precision.append(clsfr5.iloc[0,0]/(clsfr5.iloc[0,0]+clsfr5.iloc[0,1]))
recall.append(clsfr5.iloc[0,0]/(clsfr5.iloc[0,0]+clsfr5.iloc[1,0]))
f1_score.append(2*clsfr5.iloc[0,0]/(2*clsfr5.iloc[0,0]+clsfr5.iloc[0,1]+clsfr5.iloc[1,0]))
accuracy.append(round(scorerf,2))
classifier_name.append("Random_Forest_Classifier")
clsfr5

from sklearn.ensemble import AdaBoostClassifier

# ### Adaboost Classifier

adaclfr = AdaBoostClassifier()
adaclfr =adaclfr.fit(tfidf_train, y_train)
predada=adaclfr.predict(tfidf_test)
scoreada=accuracy_score(y_test,predada)
print(f'Accuracy: {round(scoreada*100,2)}%')

clsfr9 = pd.crosstab(y_test, predada, rownames=['Actual'], colnames=['Predicted'], margins=True)
plt.figure(figsize=(5,5))
plt.title("Confusion Matrix (AdaBoost_Classifier)",fontsize=8)
sns.heatmap(clsfr9.iloc[:3,:3],cmap="Paired",fmt="d",annot=True)
precision.append(clsfr9.iloc[0,0]/(clsfr9.iloc[0,0]+clsfr9.iloc[0,1]))
recall.append(clsfr9.iloc[0,0]/(clsfr9.iloc[0,0]+clsfr9.iloc[1,0]))
f1_score.append(2*clsfr9.iloc[0,0]/(2*clsfr9.iloc[0,0]+clsfr9.iloc[0,1]+clsfr9.iloc[1,0]))
accuracy.append(round(scoreada,2))
classifier_name.append("AdaBoost_Classifier")
clsfr9

# ### Ridge CLassifier

ridgeclfr = RidgeClassifier()
ridgeclfr = ridgeclfr.fit(tfidf_train, y_train)
predridge=ridgeclfr.predict(tfidf_test)
scoreridge=accuracy_score(y_test,predridge)
print(f'Accuracy: {round(scoreridge*100,2)}%')

clsfr6 = pd.crosstab(y_test, predridge, rownames=['Actual'], colnames=['Predicted'], margins=True)
plt.figure(figsize=(5,5))
plt.title("Confusion Matrix (Ridge_Classifier)",fontsize=8)
sns.heatmap(clsfr6.iloc[:3,:3],cmap="Paired",fmt="d",annot=True)
precision.append(clsfr6.iloc[0,0]/(clsfr6.iloc[0,0]+clsfr6.iloc[0,1]))
recall.append(clsfr6.iloc[0,0]/(clsfr6.iloc[0,0]+clsfr6.iloc[1,0]))
f1_score.append(2*clsfr6.iloc[0,0]/(2*clsfr6.iloc[0,0]+clsfr6.iloc[0,1]+clsfr6.iloc[1,0]))
accuracy.append(round(scoreridge,2))
classifier_name.append("Ridge_Classifier")
clsfr5

# ### SVC Classifier

svc = make_pipeline(StandardScaler(with_mean=False), SVC(gamma='auto'))
svc = svc.fit(tfidf_train, y_train)
predsvc=svc.predict(tfidf_test)
scoresvc=accuracy_score(y_test,predsvc)
print(f'Accuracy: {round(scoresvc*100,2)}%')

clsfr7 = pd.crosstab(y_test, predsvc, rownames=['Actual'], colnames=['Predicted'], margins=True)
plt.figure(figsize=(5,5))
plt.title("Confusion Matrix (SVC_Classifier)",fontsize=8)
sns.heatmap(clsfr7.iloc[:3,:3],cmap="Paired",fmt="d",annot=True)
precision.append(clsfr7.iloc[0,0]/(clsfr7.iloc[0,0]+clsfr7.iloc[0,1]))
recall.append(clsfr7.iloc[0,0]/(clsfr7.iloc[0,0]+clsfr7.iloc[1,0]))
f1_score.append(2*clsfr7.iloc[0,0]/(2*clsfr7.iloc[0,0]+clsfr7.iloc[0,1]+clsfr7.iloc[1,0]))
accuracy.append(round(scoresvc,2))
classifier_name.append("SVC_Classifier")
clsfr5

# ### Voting Based Classifier

clf1 = Perceptron(tol=1e-3)
clf2 = RandomForestClassifier(n_estimators=50, random_state=1)
clf3 = RidgeClassifier()
clf4 = make_pipeline(StandardScaler(with_mean=False), SVC(gamma='auto'))
vtng = VotingClassifier(estimators=[('perceptron', clf1), ('randorforest', clf2), ('ridge', clf3), ('svc', clf4)], 
                        voting='hard')
vtng = vtng.fit(tfidf_train, y_train)
predvt=vtng.predict(tfidf_test)
scorevt=accuracy_score(y_test,predvt)
print(f'Accuracy: {round(scorevt*100,2)}%')

clsfr8 = pd.crosstab(y_test, predvt, rownames=['Actual'], colnames=['Predicted'], margins=True)
plt.figure(figsize=(5,5))
plt.title("Confusion Matrix (Voting_Classifier)",fontsize=8)
sns.heatmap(clsfr8.iloc[:3,:3],cmap="Paired",fmt="d",annot=True)
precision.append(clsfr8.iloc[0,0]/(clsfr8.iloc[0,0]+clsfr8.iloc[0,1]))
recall.append(clsfr8.iloc[0,0]/(clsfr8.iloc[0,0]+clsfr8.iloc[1,0]))
f1_score.append(2*clsfr8.iloc[0,0]/(2*clsfr8.iloc[0,0]+clsfr8.iloc[0,1]+clsfr8.iloc[1,0]))
accuracy.append(round(scorevt,2))
classifier_name.append("Voting_Classifier")
clsfr8

# ### Comparison of all ML classifiers

precision=list(precision)
recall=list(recall)
f1_score=list(f1_score)
accuracy=list(accuracy)

precision
recall
f1_score
accuracy

results=[]
results.append(precision)
# precision

results.append(recall)
# recall

results.append(f1_score)
# f1_score

results.append(accuracy)
# accuracy

# results.append(classifier_name)
# # classifier_name

results

# precision = np.array(precision)
# recall = np.array(recall)
# f1_score = np.array(f1_score)
# accuracy = np.array(accuracy)

# print(len(precision))
# print(len(recall))
# print(len(f1_score))
# print(len(accuracy))
print(len(results))

# ### Visualisation of the ML classifiers results

precision, recall, f1_score ,accuracy = results

indices = np.arange(len(results))
results = [[x[i] for x in results] for i in range(4)]

plt.figure(figsize=(18, 20))
plt.title("Performance Metrics")

plt.barh(indices +.5 , precision, .1, label="precision",color='navy')

plt.barh(indices +.6, recall, .1, label="recall",color='c')

plt.barh(indices +.7, f1_score, .1, label="f1_score", color='darkorange')

plt.barh(indices +.8, accuracy, .1, label="accuracy", color='red')

plt.yticks(())
plt.legend(loc='upper right')
plt.subplots_adjust(left=.25)
plt.subplots_adjust(right=.95)

plt.subplots_adjust(top=.65)
plt.subplots_adjust(bottom=.45)

for i, c in zip(indices, classifier_name):
    plt.text(-.1,i, c)
plt.savefig("Performance metrics")
plt.show()

